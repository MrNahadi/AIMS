{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Exploration and Cleaning\n",
                "\n",
                "This notebook performs exploratory data analysis (EDA) on the marine engine fault dataset.\n",
                "\n",
                "**Objectives:**\n",
                "- Load and profile the dataset\n",
                "- Check for missing values and data types\n",
                "- Visualize fault label distribution\n",
                "- Analyze sensor feature distributions\n",
                "- Identify outliers\n",
                "- Document cleaning decisions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set visualization style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the marine engine fault dataset\n",
                "df = pd.read_csv('../data/marine_engine_fault_dataset.csv')\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"Total records: {len(df):,}\")\n",
                "print(f\"Total features: {df.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display first few rows\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset Profiling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check data types\n",
                "print(\"Data Types:\")\n",
                "print(df.dtypes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "print(\"Missing Values:\")\n",
                "missing_values = df.isnull().sum()\n",
                "missing_percent = (missing_values / len(df)) * 100\n",
                "missing_df = pd.DataFrame({\n",
                "    'Missing Count': missing_values,\n",
                "    'Percentage': missing_percent\n",
                "})\n",
                "print(missing_df[missing_df['Missing Count'] > 0])\n",
                "\n",
                "if missing_df['Missing Count'].sum() == 0:\n",
                "    print(\"\\n✓ No missing values found in the dataset!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check unique counts for all columns\n",
                "print(\"Unique Value Counts:\")\n",
                "unique_counts = df.nunique().sort_values(ascending=False)\n",
                "print(unique_counts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Fault Label Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define fault label mapping\n",
                "FAULT_LABELS = {\n",
                "    0: \"Normal\",\n",
                "    1: \"Fuel Injection Fault\",\n",
                "    2: \"Cooling System Fault\",\n",
                "    3: \"Turbocharger Fault\",\n",
                "    4: \"Bearing Wear\",\n",
                "    5: \"Lubrication Oil Degradation\",\n",
                "    6: \"Air Intake Restriction\",\n",
                "    7: \"Vibration Anomaly\"\n",
                "}\n",
                "\n",
                "# Count distribution\n",
                "fault_counts = df['Fault_Label'].value_counts().sort_index()\n",
                "fault_percentages = (fault_counts / len(df)) * 100\n",
                "\n",
                "print(\"Fault Label Distribution:\")\n",
                "for label, count in fault_counts.items():\n",
                "    print(f\"{label} - {FAULT_LABELS[label]}: {count:,} ({fault_percentages[label]:.2f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize fault label distribution\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Bar chart with counts\n",
                "fault_counts.plot(kind='bar', ax=ax1, color='steelblue', edgecolor='black')\n",
                "ax1.set_title('Fault Label Distribution (Counts)', fontsize=14, fontweight='bold')\n",
                "ax1.set_xlabel('Fault Label', fontsize=12)\n",
                "ax1.set_ylabel('Count', fontsize=12)\n",
                "ax1.set_xticklabels([FAULT_LABELS[i] for i in range(8)], rotation=45, ha='right')\n",
                "ax1.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add count labels on bars\n",
                "for i, v in enumerate(fault_counts):\n",
                "    ax1.text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
                "\n",
                "# Pie chart with percentages\n",
                "colors = plt.cm.Set3(range(8))\n",
                "ax2.pie(fault_counts, labels=[FAULT_LABELS[i] for i in range(8)], autopct='%1.1f%%',\n",
                "        startangle=90, colors=colors)\n",
                "ax2.set_title('Fault Label Distribution (Percentages)', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Check for class imbalance\n",
                "max_class = fault_percentages.max()\n",
                "min_class = fault_percentages.min()\n",
                "imbalance_ratio = max_class / min_class\n",
                "\n",
                "print(f\"\\nClass Balance Analysis:\")\n",
                "print(f\"Most common class: {max_class:.2f}%\")\n",
                "print(f\"Least common class: {min_class:.2f}%\")\n",
                "print(f\"Imbalance ratio: {imbalance_ratio:.2f}x\")\n",
                "\n",
                "if imbalance_ratio < 2:\n",
                "    print(\"✓ Dataset is well-balanced!\")\n",
                "else:\n",
                "    print(\"⚠ Dataset shows class imbalance - consider stratified sampling\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Sensor Feature Analysis\n",
                "\n",
                "Analyzing the 18 sensor features to understand their distributions and identify potential issues."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define sensor feature columns (excluding Timestamp and Fault_Label)\n",
                "sensor_features = [\n",
                "    'Shaft_RPM', 'Engine_Load', 'Fuel_Flow', 'Air_Pressure', 'Ambient_Temp',\n",
                "    'Oil_Temp', 'Oil_Pressure', 'Vibration_X', 'Vibration_Y', 'Vibration_Z',\n",
                "    'Cylinder1_Pressure', 'Cylinder1_Exhaust_Temp',\n",
                "    'Cylinder2_Pressure', 'Cylinder2_Exhaust_Temp',\n",
                "    'Cylinder3_Pressure', 'Cylinder3_Exhaust_Temp',\n",
                "    'Cylinder4_Pressure', 'Cylinder4_Exhaust_Temp'\n",
                "]\n",
                "\n",
                "print(f\"Total sensor features: {len(sensor_features)}\")\n",
                "print(f\"Features: {sensor_features}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Histograms for All Sensor Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate histograms for all 18 sensor features\n",
                "fig, axes = plt.subplots(6, 3, figsize=(18, 20))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, feature in enumerate(sensor_features):\n",
                "    ax = axes[idx]\n",
                "    df[feature].hist(bins=50, ax=ax, color='skyblue', edgecolor='black', alpha=0.7)\n",
                "    ax.set_title(f'{feature}', fontsize=11, fontweight='bold')\n",
                "    ax.set_xlabel('Value', fontsize=9)\n",
                "    ax.set_ylabel('Frequency', fontsize=9)\n",
                "    ax.grid(axis='y', alpha=0.3)\n",
                "    \n",
                "    # Add mean and median lines\n",
                "    mean_val = df[feature].mean()\n",
                "    median_val = df[feature].median()\n",
                "    ax.axvline(mean_val, color='red', linestyle='--', linewidth=1.5, label=f'Mean: {mean_val:.2f}')\n",
                "    ax.axvline(median_val, color='green', linestyle='--', linewidth=1.5, label=f'Median: {median_val:.2f}')\n",
                "    ax.legend(fontsize=8)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Distribution of All Sensor Features', fontsize=16, fontweight='bold', y=1.001)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Boxplots to Identify Outliers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate boxplots for all 18 sensor features\n",
                "fig, axes = plt.subplots(6, 3, figsize=(18, 20))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, feature in enumerate(sensor_features):\n",
                "    ax = axes[idx]\n",
                "    df.boxplot(column=feature, ax=ax, patch_artist=True,\n",
                "               boxprops=dict(facecolor='lightblue', color='black'),\n",
                "               medianprops=dict(color='red', linewidth=2),\n",
                "               whiskerprops=dict(color='black'),\n",
                "               capprops=dict(color='black'),\n",
                "               flierprops=dict(marker='o', markerfacecolor='orange', markersize=3, alpha=0.5))\n",
                "    ax.set_title(f'{feature}', fontsize=11, fontweight='bold')\n",
                "    ax.set_ylabel('Value', fontsize=9)\n",
                "    ax.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Boxplots for Outlier Detection', fontsize=16, fontweight='bold', y=1.001)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quantify outliers using IQR method\n",
                "print(\"Outlier Analysis (IQR Method):\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "outlier_summary = []\n",
                "\n",
                "for feature in sensor_features:\n",
                "    Q1 = df[feature].quantile(0.25)\n",
                "    Q3 = df[feature].quantile(0.75)\n",
                "    IQR = Q3 - Q1\n",
                "    \n",
                "    lower_bound = Q1 - 1.5 * IQR\n",
                "    upper_bound = Q3 + 1.5 * IQR\n",
                "    \n",
                "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
                "    outlier_count = len(outliers)\n",
                "    outlier_percent = (outlier_count / len(df)) * 100\n",
                "    \n",
                "    outlier_summary.append({\n",
                "        'Feature': feature,\n",
                "        'Outlier Count': outlier_count,\n",
                "        'Percentage': outlier_percent,\n",
                "        'Lower Bound': lower_bound,\n",
                "        'Upper Bound': upper_bound\n",
                "    })\n",
                "\n",
                "outlier_df = pd.DataFrame(outlier_summary)\n",
                "outlier_df = outlier_df.sort_values('Outlier Count', ascending=False)\n",
                "print(outlier_df.to_string(index=False))\n",
                "\n",
                "total_outliers = outlier_df['Outlier Count'].sum()\n",
                "print(f\"\\nTotal outlier instances across all features: {total_outliers:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Correlation Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute correlation matrix for sensor features\n",
                "correlation_matrix = df[sensor_features].corr()\n",
                "\n",
                "# Visualize correlation heatmap\n",
                "plt.figure(figsize=(16, 14))\n",
                "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0,\n",
                "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
                "plt.title('Correlation Matrix of Sensor Features', fontsize=16, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Identify highly correlated feature pairs\n",
                "print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "high_corr_pairs = []\n",
                "for i in range(len(correlation_matrix.columns)):\n",
                "    for j in range(i+1, len(correlation_matrix.columns)):\n",
                "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
                "            high_corr_pairs.append({\n",
                "                'Feature 1': correlation_matrix.columns[i],\n",
                "                'Feature 2': correlation_matrix.columns[j],\n",
                "                'Correlation': correlation_matrix.iloc[i, j]\n",
                "            })\n",
                "\n",
                "if high_corr_pairs:\n",
                "    high_corr_df = pd.DataFrame(high_corr_pairs)\n",
                "    high_corr_df = high_corr_df.sort_values('Correlation', ascending=False, key=abs)\n",
                "    print(high_corr_df.to_string(index=False))\n",
                "else:\n",
                "    print(\"No highly correlated feature pairs found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Data Cleaning Decisions\n",
                "\n",
                "### Summary of Findings:\n",
                "\n",
                "1. **Missing Values**: The dataset has no missing values, which is excellent for model training.\n",
                "\n",
                "2. **Data Types**: All sensor features are numeric (float64), and Fault_Label is integer - appropriate for ML.\n",
                "\n",
                "3. **Class Balance**: The fault label distribution shows whether classes are balanced or if stratified sampling is needed.\n",
                "\n",
                "4. **Outliers**: Outliers detected using IQR method. These are likely legitimate extreme operating conditions rather than errors, so we will retain them.\n",
                "\n",
                "5. **Feature Correlations**: Some cylinder measurements may be correlated, which is expected for similar engine components.\n",
                "\n",
                "### Cleaning Decisions:\n",
                "\n",
                "- **No data removal**: All records are valid with no missing values\n",
                "- **Retain outliers**: Outliers represent real fault conditions and extreme operating states\n",
                "- **Keep all features**: All 18 sensor features provide valuable information\n",
                "- **Timestamp handling**: Will drop Timestamp column for modeling (not a predictive feature)\n",
                "- **Stratified splitting**: Will use stratified train-test split to maintain class balance\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "The dataset is clean and ready for preprocessing. The next notebook will handle:\n",
                "- Feature-target separation\n",
                "- Train-test splitting (stratified)\n",
                "- Feature scaling using StandardScaler\n",
                "- Saving the preprocessor for deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final dataset info\n",
                "print(\"Final Dataset Summary:\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"Total records: {len(df):,}\")\n",
                "print(f\"Total features: {len(sensor_features)}\")\n",
                "print(f\"Target classes: {df['Fault_Label'].nunique()}\")\n",
                "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
                "print(f\"\\n✓ Dataset is clean and ready for preprocessing!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
